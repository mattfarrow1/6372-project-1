---
title: "Project 1 EDA"
author: "MB"
date: "9/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r import data}

life <-
  read.csv(
    "C:/Users/megan/OneDrive/Grad school/DS6372/ProjectDetails_2_2_2_2_2/Life Expectancy Data.csv"
  )
head(life)
str(life)
summary(life)

#visualize distribution of life expectancy
hist(life$Life.expectancy)
qqnorm(life$Life.expectancy)

```

There are a lot of NA's in this data set. Make a new data set without them.

```{r}
library(tidyverse)

life$Status <- as.factor(life$Status)
life$Country <- as.factor(life$Country)

colSums(is.na(life))

#if we want to replace with the mean value instead...
#ds$ColumnName[is.na(ds$ColumnName)] <- mean(ds$ColumnName,na.rm = TRUE)

```


```{r}


library(dplyr)
life %>% 
  group_by(Status) %>% 
  summarize(count = n(),
            avg_lifexp = mean(Life.expectancy, na.rm = TRUE),
            avg_infmort = mean(infant.deaths, na.rm = TRUE),
            avg_admort = mean(Adult.Mortality, na.rm = TRUE))

life %>% 
    group_by(Country) %>% 
    summarize(n = n(),
             avg_lifexp = mean(Life.expectancy),
             avg_infmort = mean(infant.deaths),
             avg_admort = mean(Adult.Mortality))

```

```{r echo = FALSE}
#scatterplot matrix

library(GGally)

ggpairs(life, columns = 2:5)
ggpairs(life, columns = 6:10)
ggpairs(life, columns = 11:15)
ggpairs(life, columns = 16:22)

```

Adult Mortality looks like two different groupings (possibly developed vs developing?)

Several of the distributions of the other variables are definitely non-normal

Thinness of 1.19 and 5.9 are very highly correlated to each other

```{r}
ggpairs(life, columns = 2:5, mapping = aes(color = Status))

```

Check correlation matrix for all variables
```{r}
life <- janitor::clean_names(life)
colnames(life)
ggcorr(life, label = TRUE, label_alpha = TRUE, layout.exp = 2)
```

```{r}
#remove redundant variables - pick ones with less NA

life_1 <- select(life, -c(under_five_deaths, gdp, thinness_1_19_years))
colnames(life_1)
ggcorr(life_1, label = TRUE, label_alpha = TRUE, layout.exp = 2)

colSums(is.na(life_1))

#remove all rows with NA life expectancy and population and limit scope to not include those countries
life_1 <- life_1 %>%
  drop_na(life_expectancy, population)
colSums(is.na(life_1))
```

```{r}
#check for outliers in each distribution of each variable to determine how to handle imputing other values

for (col in 4:ncol(life_1)) {
    hist(life_1[,col], main=names(life_1[col]))
}

```


```{r}
#impute values
#alcohol & thinness- set NA to 0
#hep_b, polio, total_exp, dip - set to median
#bmi - set NA to mean

#create new data set for imputation
life_2 <- life_1

#my very inefficient way to accomplish this :)
life_2$hepatitis_b[is.na(life_2$hepatitis_b)] <- median(life_2$hepatitis_b,na.rm = TRUE)
life_2$polio[is.na(life_2$polio)] <- median(life_2$polio,na.rm = TRUE)
life_2$total_expenditure[is.na(life_2$total_expenditure)] <- median(life_2$total_expenditure,na.rm = TRUE)
life_2$diphtheria[is.na(life_2$diphtheria)] <- median(life_2$diphtheria,na.rm = TRUE)
life_2$bmi[is.na(life_2$bmi)] <- mean(life_1$bmi,na.rm = TRUE)
life_2$alcohol[is.na(life_2$alcohol)] <- 0
life_2$thinness_5_9_years[is.na(life_2$thinness_5_9_years)] <- 0

#any more NAs?
colSums(is.na(life_2))
#we now have 2284 obs with 19 vars

```


```{r}
#check new histograms to see if any shifts
for (col in 4:ncol(life_2)) {
    hist(life_2[,col], main=names(life_2[col]))
}

#save new dataset to csv
write.csv(life_2, file = "life_imputed.csv")
write.csv(life_1, file = "life_NA_pop_LE.csv")

```


```{r}
##################
# Train/test sets
##################
library(caret)

#standardize the data to prep for KNN first - everything except life expectancy
preProcValues <- preProcess(life_2[,-4], method = c("scale"))

life_3 <- predict(preProcValues, life_2)

#visualize the scaling
for (col in 4:ncol(life_3)) {
    hist(life_3[,col], main=names(life_3[col]))
}

#split the data

# set random seed
set.seed(123) 
# create the training partition that is 75% of total obs
inTraining <- createDataPartition(life_3$life_expectancy, p=0.75, list=FALSE)
# create training/testing dataset
trainSet <- life_3[inTraining,]   
testSet <- life_3[-inTraining,]   
# verify number of obs 
nrow(trainSet)
nrow(testSet) 


```


```{r}

set.seed(567)
ctrl <- trainControl(method="repeatedcv",repeats = 5, returnResamp = "all") 

knnFit <- train(life_expectancy ~ ., data = trainSet, method = "knn",  trControl = ctrl, tuneLength = 10)
knnFit
```


```{r}

#check metrics on test set
Predictions_knn5 <- predict(knnFit,newdata=testSet)
#performance measurement
postResample(testSet$life_expectancy,Predictions_knn5)

trellis.par.set(caretTheme())
plot(knnFit)  
plot(knnFit, metric = "Rsquared")
plot(knnFit, metric = "MAE")

```


```{r}
#try knn with k less than 5
knnFit2 <- train(life_expectancy ~ ., data = trainSet, method = "knn",  trControl = ctrl, tuneGrid = expand.grid(k = c(1,3,5)))
knnFit2

```

```{r}

#check metrics on test set
Predictions_knn3 <- predict(knnFit2,newdata=testSet)
#performance measurement
postResample(testSet$life_expectancy,Predictions_knn3)

trellis.par.set(caretTheme())
plot(knnFit2)  
plot(knnFit2, metric = "Rsquared")
plot(knnFit2, metric = "MAE")

```

##Testing with Trees

``` {r}
install.packages("tree")
library(tree)
#have to remove country because this tree function has max 32 levels
tree1 <- tree(life_expectancy ~., data = trainSet[,-1])
summary(tree1)
plot(tree1)

```

``` {r}
#improve tree performance

cv.tree1 <- cv.tree(tree1)
plot(cv.tree1$size,cv.tree1$dev,type='b')
```
``` {r}
#check predictions
yhat = predict(tree1,newdata=testSet[,-1])
plot(yhat,testSet$life_expectancy)
mean((yhat - testSet$life_expectancy)^2)
```