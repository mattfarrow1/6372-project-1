---
title: "Project 1 EDA"
author: "MB"
date: "9/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r import data}

life <-
  read.csv(
    "C:/Users/megan/OneDrive/Grad school/DS6372/ProjectDetails_2_2_2_2_2/Life Expectancy Data.csv"
  )
head(life)
str(life)
summary(life)

#visualize distribution of life expectancy
hist(life$Life.expectancy)
qqnorm(life$Life.expectancy)

```

There are a lot of NA's in this data set. Make a new data set without them.

```{r}
library(tidyverse)

life$Status <- as.factor(life$Status)
life$Country <- as.factor(life$Country)

colSums(is.na(life))

#if we want to replace with the mean value instead...
#ds$ColumnName[is.na(ds$ColumnName)] <- mean(ds$ColumnName,na.rm = TRUE)

```


```{r}


library(dplyr)
life %>% 
  group_by(Status) %>% 
  summarize(count = n(),
            avg_lifexp = mean(Life.expectancy, na.rm = TRUE),
            avg_infmort = mean(infant.deaths, na.rm = TRUE),
            avg_admort = mean(Adult.Mortality, na.rm = TRUE))

life %>% 
    group_by(Country) %>% 
    summarize(n = n(),
             avg_lifexp = mean(Life.expectancy),
             avg_infmort = mean(infant.deaths),
             avg_admort = mean(Adult.Mortality))

```

```{r echo = FALSE}
#scatterplot matrix

library(GGally)

ggpairs(life, columns = 2:5)
ggpairs(life, columns = 6:10)
ggpairs(life, columns = 11:15)
ggpairs(life, columns = 16:22)

```

Adult Mortality looks like two different groupings (possibly developed vs developing?)

Several of the distributions of the other variables are definitely non-normal

Thinness of 1.19 and 5.9 are very highly correlated to each other

```{r}
ggpairs(life, columns = 2:5, mapping = aes(color = Status))

```

Check correlation matrix for all variables
```{r}
life <- janitor::clean_names(life)
colnames(life)
ggcorr(life, label = TRUE, label_alpha = TRUE, layout.exp = 2)
```

```{r}
#remove redundant variables - pick ones with less NA

life_1 <- select(life, -c(under_five_deaths, gdp, thinness_1_19_years))
colnames(life_1)
ggcorr(life_1, label = TRUE, label_alpha = TRUE, layout.exp = 2)

colSums(is.na(life_1))

#remove all rows with NA life expectancy and population and limit scope to not include those countries
life_1 <- life_1 %>%
  drop_na(life_expectancy, population)
colSums(is.na(life_1))
```

```{r}
#check for outliers in each distribution of each variable to determine how to handle imputing other values

for (col in 4:ncol(life_1)) {
    hist(life_1[,col], main=names(life_1[col]))
}

```


```{r}
#impute values
#alcohol & thinness- set NA to 0
#hep_b, polio, total_exp, dip - set to median
#bmi - set NA to mean

#create new data set for imputation
life_2 <- life_1

#my very inefficient way to accomplish this :)
life_2$hepatitis_b[is.na(life_2$hepatitis_b)] <- median(life_2$hepatitis_b,na.rm = TRUE)
life_2$polio[is.na(life_2$polio)] <- median(life_2$polio,na.rm = TRUE)
life_2$total_expenditure[is.na(life_2$total_expenditure)] <- median(life_2$total_expenditure,na.rm = TRUE)
life_2$diphtheria[is.na(life_2$diphtheria)] <- median(life_2$diphtheria,na.rm = TRUE)
life_2$bmi[is.na(life_2$bmi)] <- mean(life_1$bmi,na.rm = TRUE)
life_2$alcohol[is.na(life_2$alcohol)] <- 0
life_2$thinness_5_9_years[is.na(life_2$thinness_5_9_years)] <- 0

#any more NAs?
colSums(is.na(life_2))
#we now have 2284 obs with 19 vars

```


```{r}
#check new histograms to see if any shifts
for (col in 4:ncol(life_2)) {
    hist(life_2[,col], main=names(life_2[col]))
}

#save new dataset to csv
write.csv(life_2, file = "life_imputed.csv")
write.csv(life_1, file = "life_NA_pop_LE.csv")

```


```{r}
##################
# Train/test sets
##################
library(caret)
library(readr)

#load data
life_2 <- read.csv(file = "life_imputed.csv", header = TRUE) %>%
  select(2:20)
life_1 <- read.csv(file = "life_NA_pop_LE.csv", header = TRUE) %>%
  select(2:20)

#standardize the data to prep for KNN first - everything except life expectancy
preProcValues <- preProcess(life_2[,-4], method = c("scale"))

life_3 <- predict(preProcValues, life_2)

#save standardized data set
write.csv(life_3, file = "life_standard.csv")
#load back in
life_3 <- read.csv(file = "life_standard.csv", header=TRUE) %>%
  select(2:20)

#visualize the scaling
for (col in 4:ncol(life_3)) {
    hist(life_3[,col], main=names(life_3[col]))
}

#split the data

# set random seed
set.seed(123) 
# create the training partition that is 75% of total obs
inTraining <- createDataPartition(life_3$life_expectancy, p=0.75, list=FALSE)
# create training/testing dataset
trainSet <- life_3[inTraining,]   
testSet <- life_3[-inTraining,]   
# verify number of obs 
nrow(trainSet)
nrow(testSet) 

#split non-standardized data for trees
set.seed(123) 
# create the training partition that is 75% of total obs
inTraining2 <- createDataPartition(life_2$life_expectancy, p=0.75, list=FALSE)
# create training/testing dataset
trainSet2 <- life_2[inTraining2,]   
testSet2 <- life_2[-inTraining2,]   
# verify number of obs 
nrow(trainSet2)
nrow(testSet2) 


#split the complete data set without imputation for linear regression & variable selection **HAS NA**
set.seed(123) 
# create the training partition that is 75% of total obs
inTraining3 <- createDataPartition(life_1$life_expectancy, p=0.75, list=FALSE)
# create training/testing dataset
trainSet3 <- life_1[inTraining3,]   
testSet3 <- life_1[-inTraining3,]   
# verify number of obs 
nrow(trainSet3)
nrow(testSet3) 

```

```{r}
library(leaps)
#testing out forward selection
reg.fwd=regsubsets(life_expectancy~.,data=trainSet2,method="forward",nvmax=5)
reg.fwd

#remove country because this looks like it is causing issues
reg.fwd=regsubsets(life_expectancy~.,data=trainSet2[,-1],method="forward",nvmax=18)
reg.fwd

#remove country because this looks like it is causing issues
reg.back=regsubsets(life_expectancy~.,data=trainSet2[,-1],method="backward",nvmax=18)
reg.back

```

```{r, echo=T}
#Really handy predict function
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```

## Predictions for Forward Selection

```{r, echo=T, fig.width=5,fig.height=4}
testASE.fwd<-c()
for (i in 1:17){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=testSet2,id=i) 
  testASE.fwd[i]<-mean((testSet2$life_expectancy-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:17,testASE.fwd,type="l",xlab="# of predictors",ylab="test vs train ASE")
index<-which(testASE.fwd==min(testASE.fwd))
points(index,testASE.fwd[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
lines(1:17,rss/570,lty=3,col="blue")  #Dividing by 570 since ASE=RSS/sample size
```

## Predictions for Backward Selection

```{r, echo=T, fig.width=5,fig.height=4}
testASE.back<-c()
#note my index is to 20 since that what I set it in regsubsets
for (i in 1:17){
  predictions<-predict.regsubsets(object=reg.back,newdata=testSet2,id=i) 
  testASE.back[i]<-mean((testSet$life_expectancy-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:17,testASE.back,type="l",xlab="# of predictors",ylab="test vs train ASE")
index<-which(testASE.back==min(testASE.back))
points(index,testASE.back[index],col="red",pch=10)
rss<-summary(reg.back)$rss
lines(1:17,rss/570,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size
```
Both forward and backward selection say to use all the predictors.

##LASSO

```{r}
#LASSO
library(glmnet)
#Formatting data for GLM net
x=model.matrix(life_expectancy~.,trainSet2)[,-1]
y=trainSet2$life_expectancy

xtest<-model.matrix(life_expectancy~.,testSet2)[,-1]
ytest<-testSet2$life_expectancy



grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
bestlambda
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO
```

```{r, echo=T}
coef(lasso.mod,s=bestlambda)
```

##Linear Model for Prediction

```{r}
#use everything except country
lm4pred <- lm(life_expectancy ~., data = trainSet2[,-1])
summary(lm4pred)

```
```{r}
#visualize model fit
plot(lm4pred$fitted.values,trainSet2$life_expectancy,xlab="Predicted",ylab="Life Expectancy")
abline(0,1)

linear_prediction <- predict(lm4pred, testSet2)
plot(linear_prediction, testSet2$life_expectancy)
abline(0,1)

#metrics
testASE.lmpred<-mean((testSet2$life_expectancy-linear_prediction)^2)
testASE.lmpred

```

```


```{r}

set.seed(567)
ctrl <- trainControl(method="repeatedcv",repeats = 5, returnResamp = "all") 

knnFit <- train(life_expectancy ~ ., data = trainSet, method = "knn",  trControl = ctrl, tuneLength = 10)
knnFit
```


```{r}

#check metrics on test set
Predictions_knn5 <- predict(knnFit,newdata=testSet)
#performance measurement
postResample(testSet$life_expectancy,Predictions_knn5)

trellis.par.set(caretTheme())
plot(knnFit)  
plot(knnFit, metric = "Rsquared")
plot(knnFit, metric = "MAE")

```


```{r}
#try knn with k less than 5
knnFit2 <- train(life_expectancy ~ ., data = trainSet, method = "knn",  trControl = ctrl, tuneGrid = expand.grid(k = c(1,3,5)))
knnFit2

```

```{r}

#check metrics on test set
Predictions_knn3 <- predict(knnFit2,newdata=testSet)
#performance measurement
postResample(testSet$life_expectancy,Predictions_knn3)

trellis.par.set(caretTheme())
plot(knnFit2)  
plot(knnFit2, metric = "Rsquared")
plot(knnFit2, metric = "MAE")

```

##Testing with Trees

``` {r}
#install.packages("tree")
library(tree)
#have to remove country because this tree function has max 32 levels
tree1 <- tree(life_expectancy ~., data = trainSet2[,-1])
summary(tree1)
plot(tree1)

```

``` {r}
#check tree performance

cv.tree1 <- cv.tree(tree1)
plot(cv.tree1$size,cv.tree1$dev,type='b')
```
``` {r}
#check predictions
yhat = predict(tree1,newdata=testSet2[,-1])
plot(yhat,testSet2$life_expectancy)
abline(0,1)
mean((yhat - testSet2$life_expectancy)^2)
```


```{r}
#try random forest compared to single tree model
#install.packages("randomForest")
library(randomForest)
set.seed(1)
#have to remove country again
rfFit <- randomForest(life_expectancy ~., data = trainSet2[,-1])
rfFit
```


```{r}
#testing using all predictors for each tree
set.seed(1)
#have to remove country again
rfFit2 <- randomForest(life_expectancy ~., data = trainSet2[,-1], mtry = 17)
rfFit2

```


```{r}
#compare y-hat for the two random forest models
yhat.rf <- predict(rfFit, newdata=testSet[,-1])
yhat.rf2 <- predict(rfFit2, newdata=testSet[,-1])

plot(yhat.rf,testSet2$life_expectancy)
abline(0,1)

plot(yhat.rf2,testSet2$life_expectancy)
abline(0,1)
```


```{r}
#random forest using caret

```


```{r}


```


```{r}


```


```{r}


```


