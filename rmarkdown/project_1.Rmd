---
title: '6372: Project 1'
author: "Megan Ball, Matt Farrow, Neddy Nyatome"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(corrplot)
library(GGally)
library(gt)
library(hrbrthemes)
library(car)
library(leaps)
```

## Introduction

Using the World Health Organization (WHO) data compiled by Kumar Rajarshi, Deeksha Russell, and Duan Wang, we attempted to predict which factors affect life expectancy.

## Data Description

Description and context of the Life Expectancy (WHO) dataset can be found [here](https://www.kaggle.com/kumarajarshi/life-expectancy-who). Data has been compiled from several different data sets into a final data set that represents health factors for 193 countries between the years of 2000-2015.

```{r load-data, echo=FALSE, message=FALSE}
# Load data
df <- read_csv(here::here("data - raw", "Life Expectancy Data.csv"))

# Clean up column names
df_clean <- janitor::clean_names(df)

# Look at the data
glimpse(df_clean)
```

Looking at the data, there are 2,938 observatrions and 22 variables that cover various social, economic, and health-related factors. 

## Exploratory Data Analysis

We'll start by taking a closer look at life expectancy. 

```{r echo=FALSE, message=FALSE}
df_clean %>% 
  ggplot(aes(life_expectancy)) +
  geom_histogram(fill = "steelblue", color = "black") +
  labs(title = "Histogram of Life Expectancy",
       x = "Age",
       y = "Count") +
  theme_ipsum()

df_clean %>% 
  ggplot(aes(sample = life_expectancy)) +
  geom_qq(pch = 21, size = 3, na.rm = TRUE) +
  geom_qq_line(color = "indianred", na.rm = TRUE) +
  labs(title = "Quantile-Quantile Plot of Life Expectancy",
       x = "Theoretical",
       y = "Sample") +
  theme_ipsum()
```

```{r convert-to-factor, echo=FALSE}
# Convert country and status to factors
df_clean$country <- as_factor(df_clean$country)
df_clean$status  <- as_factor(df_clean$status)
```

### Check for Correlation

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggcorr(
  df_clean,
  label = TRUE,
  label_alpha = TRUE,
  label_size = 3,
  layout.exp = 2,
  cex = 3.5,
  hjust = 1
)
```

We examined various correlation matrices to determine whether there were variable(s) that should be addressed. We'll start by removing redundant variables and picking ones with fewer `NA` values.

```{r message=FALSE, warning=FALSE, echo=FALSE}
df_clean %>% 
  select(-c(under_five_deaths, gdp, thinness_1_19_years)) %>% 
  ggcorr(
  label = TRUE,
  label_alpha = TRUE,
  label_size = 3,
  layout.exp = 2,
  cex = 3.5,
  hjust = 1
)
```

Let's now look at what happens when we also remove `population`, since it has minimal correlation to `life_expectancy`.

```{r message=FALSE, warning=FALSE, echo=FALSE}
df_clean %>% 
  select(-c(under_five_deaths, gdp, thinness_1_19_years, population)) %>% 
  ggcorr(
  label = TRUE,
  label_alpha = TRUE,
  label_size = 3,
  layout.exp = 2,
  cex = 3.5,
  hjust = 1
)
```

We'll proceed, dropping the columns we've looked at.

```{r echo=FALSE}
df_clean <- df_clean %>% 
  select(-c(under_five_deaths, gdp, thinness_1_19_years, population))
```

### Deal with Missing Values

Check for missing values.

```{r echo=FALSE}
# Check for missing values
tibble(variable = names(colSums(is.na(df_clean))),
       missing = colSums(is.na(df_clean))) %>% 
  gt()
```

Drop all rows where life expectancy is `NA`

```{r}
# Drop all rows where life expectancy is NA
df_clean <- df_clean %>% 
  filter(!is.na(life_expectancy))

# Recheck missing value counts
tibble(variable = names(colSums(is.na(df_clean))),
       missing = colSums(is.na(df_clean))) %>% 
  gt()
```

Drop remaining rows with NA's for the interpretable model

```{r}
# Drop remaining rows with NA's for the interpretable model
# Remove all rows with an NA
df_interp <- na.omit(df_clean)

# Final check of missing values
tibble(variable = names(colSums(is.na(df_interp))),
       missing = colSums(is.na(df_interp))) %>% 
  gt()
```

### Analysis of Years

Now that we've subsetted our variables and dealt with `NA`'s, let's look at how many records we now have by year.

```{r echo=FALSE}
df_interp %>% count(year)
```

Our feature engineering dropped a number of records from 2000 and 2001, and almost all of the records from 2015. Let's go with the most recent "good" sample size (2011-2014).

```{r}
df_interp <- df_interp %>% filter(year %in% 2011:2014)
```

## Build the Model

Now that we've finished our feature engineering, let's start building our interpretable model. 

```{r echo=FALSE}
# Set the maximum number of variables to consider in the model. Although the
# model can handle up to 20, the more we add, the less interpretable the final
# model will be.
consider <- 18

# Fit the model. We'll remove country, but keep it in the data set for
# interpretation.
regfit_full <-
  regsubsets(life_expectancy ~ .,
             df_interp[, -1],
             nvmax = consider)

# Examine the regression summary
reg_summary <- summary(regfit_full)
```

```{r}
# Look at the names of reg_summary
names(reg_summary)

# What are the R-squared values?
reg_summary$rsq
```

### Plot the RSS, adjusted $R^2$, $C_p$, and BIC for all Models

```{r echo=FALSE}
par(mfrow = c(2, 2))
plot(reg_summary$rss,
     xlab = "Number of Variables",
     ylab = "RSS",
     type = "l")
plot(reg_summary$adjr2,
     xlab = "Number of Variables",
     ylab = "Adjusted RSq",
     type = "l")
which.max(reg_summary$adjr2)
points(
  11,
  reg_summary$adjr2[11],
  col = "red",
  cex = 2,
  pch = 20
)
plot(reg_summary$cp,
     xlab = "Number of Variables",
     ylab = "Cp",
     type = "l")
points(
  10,
  reg_summary$cp[which.min(reg_summary$cp)],
  col = "red",
  cex = 2,
  pch = 20
)
plot(reg_summary$bic,
     xlab = "Number of Variables",
     ylab = "BIC",
     type = "l")
points(
  5,
  reg_summary$bic[which.min(reg_summary$bic)],
  col = "red",
  cex = 2,
  pch = 20
)
```

Now we'll look at the selected variables for the best model with a given number of predictors.

```{r echo=FALSE}
plot(regfit_full, scale = "r2")
plot(regfit_full, scale = "adjr2")
plot(regfit_full, scale = "Cp")
plot(regfit_full, scale = "bic")
```

The model with the lowest BIC is the five-variable model that contains `adult_mortality`, `percentage_expenditure`, `total_expenditure`, `hiv_aids`, and `income_composition_of_resources`. We can use the `coef()` function to see the coefficient estimates associated with this model.

```{r echo=FALSE}
coef(regfit_full, 5)
```

### Run Forward and Backward Stepwise Selection

```{r echo=FALSE}
# Forward
regfit_fwd <-
  regsubsets(life_expectancy ~ ., data = df_interp[, -1], method = "forward")
summary(regfit_fwd)

# Backward
regfit_bwd <-
  regsubsets(life_expectancy ~ ., data = df_interp[, -1], method = "backward")
summary(regfit_bwd)

# Compare coefficients
tibble(variables = names(coef(regfit_full, 5)),
       full = coef(regfit_full, 5),
       fwd = coef(regfit_fwd, 5),
       bwd = coef(regfit_bwd, 5)) %>% 
  gt()
```

### Choose Using Validation Set

```{r warning=FALSE, echo=FALSE}
# Set seed
set.seed(1)

# Build test and training data sets, dropping country from regfit_best &
# test_mat because I was getting a warning that 1 linear dependencies found
train <- sample(c(TRUE, FALSE), nrow(df_interp), rep = TRUE)
test <- (!train)
regfit_best <- regsubsets(life_expectancy ~ ., data = df_interp[train, -1])
test_mat <- model.matrix(life_expectancy ~ ., data = df_interp[test, -1])

# Run a loop, and for each size `i`, extract the coefficients from `regfit_best`
# for the best model of that size, multiply them into the appropriate columns of
# the test model matric to form the predictions, and compute the test MSE.
val_errors <- rep(NA, consider)
for (i in 1:8) {
  coefi <- coef(regfit_best, id = i)
  pred <- test_mat[, names(coefi)] %*% coefi
  val_errors[i] <- mean((df_interp$life_expectancy[test] - pred)^2)
}
```

```{r}
# Find the best model
val_errors
coef(regfit_best, which.min(val_errors))
```

```{r echo=FALSE}
# Write a prediction fuction
predict_regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}

# Perform best subset selection on the full data set, and select the best model.
regfit_best <- regsubsets(life_expectancy ~ ., data = df_interp[,-1])
coef(regfit_best, which.min(val_errors))
```

### Choose Using Cross-Validation

```{r echo=FALSE}
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(df_interp), replace = TRUE)
cv_errors <- matrix(NA, k, consider, dimnames = list(NULL, paste(1:consider)))

predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  mat[, names(coefi)] %*% coefi
}

# Perform cross-validation
for (j in 1:k) {
  best_fit <-
    regsubsets(life_expectancy ~ ., data = df_interp[folds != j, -1], nvmax = consider)
  for (i in 1:16) {
    pred <- predict(best_fit, df_interp[folds == j, -1], id = i)
    cv_errors[j, i] <-
      mean((df_interp$life_expectancy[folds == j] - pred) ^ 2)
  }
}

# Use the apply function to average over the columns of the matrix in order to
# obtain a vector for which the jth element is the cross-validation error for
# the j-variable model.
mean_cv_errors <- apply(cv_errors, 2, mean)
mean_cv_errors
par(mfrow = c(1, 1))
plot(mean_cv_errors, type = "b")

# Perform best subset selection on the full data set in order to obtain the
# variables for the final model
reg_best <- regsubsets(life_expectancy ~ ., data = df_interp[, -1])
coef(reg_best, 4)  # the number sets the number of variables we want
```

### Build the Final Model

```{r echo=FALSE}
# Build the final model using the best subset selection results
final_model <-
  lm(
    life_expectancy ~ adult_mortality +
      total_expenditure +
      hiv_aids +
      income_composition_of_resources,
    data = df_interp
)

# Final model summary
summary(final_model)
```

### Check Diagnostics

```{r echo=FALSE}
par(mfrow = c(1, 1))
plot(
  final_model$fitted.values,
  df_interp$life_expectancy,
  xlab = "Predicted",
  ylab = "Life Expectancy"
)
lines(c(0, 90), c(0, 90), col = "red")

par(mfrow=c(2,2))
plot(final_model)
```

### Fit our Model to our Data Set

```{r echo=FALSE}
par(mfrow = c(1, 2))
test.model <- lm(life_expectancy ~ ., df_interp[, -1])
plot(test.model$fitted.values,
     test.model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals")
plot(df_interp$life_expectancy,
     test.model$residuals,
     xlab = "Life Expectancy",
     ylab = "Residuals")
```

$Life~Expectancy = 47.106 - 0.013(Adult~Mortality) + 0.289(Total~Expenditure) - 0.911(HIV/AIDS) + 36.649(Income~Composition~of~Resources)$

## KNN Model

```{r}

```



## Objective 1

### Restatement of Problem and the Overall Approach to Solve It

### Model Selection

#### Type of Selection

- LASSO
- RIDGE
- ELASTIC NET
- Stepwise
- Forward
- Backward
- Manual / Intuition
- A mix of all of the above

#### Checking Assumptions

- Residual Plots
- Influential point analysis (Cook’s D and Leverage)

#### Compare Competing Models

Via:  Training and test set split or CV
Possible Metrics: ASE (Required), AIC, BIC, adj R2, are all welcome additions
	
#### Parameter Interpretation (Simple model only)

- Interpretation 
- Confidence Intervals

#### Additional Details on a more complicated regression model 

## Objective 2 Deliverable (see above)
	
Final conclusions from the analyses of Objective 1’s interpretable model and include comments on what model would be recommended if prediction was the only goal (comparing all models considered).  

## Appendix 

### Figures

### R Code
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```
